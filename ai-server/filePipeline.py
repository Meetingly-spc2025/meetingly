import textwrap
import os
import ffmpeg
from openai import OpenAI
from dotenv import load_dotenv 
from pydub import AudioSegment
import tiktoken
import shutil

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# 1. ìŒì„± íŒŒì¼ ì¸ì½”ë”© ë³€í™˜
def convert_to_wav(input_path):
    import uuid
    
    print("[íŒŒì¼ íŒŒì´í”„ë¼ì¸ ì‹œì‘]", input_path)
   
    tmp_dir = os.path.join("tmp_audio")
    os.makedirs(tmp_dir, exist_ok=True)

    unique_name = str(uuid.uuid4())[:8] + ".wav"
    output_path = os.path.join(tmp_dir, unique_name)

    print("convert_to_wav:: ", os.path.exists(input_path))

    try:
        (
            ffmpeg
            .input(input_path)
            .output(output_path, format='wav', acodec='pcm_s16le', ac=1, ar='16000')
            .overwrite_output()
            .run(quiet=True)
        )
        print("ffmeg ì‹¤í–‰ ì™„ë£Œ")
        return output_path
    except ffmpeg.Error as e:
        print("[FFmpeg ë³€í™˜ ì˜¤ë¥˜]", e.stderr.decode())
        raise RuntimeError("ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨")
    
# ìŒì„± ë¶„í• 
def split_audio(input_path, chunk_length_ms=10 * 60 * 1000):  # 10ë¶„ ë‹¨ìœ„ë¡œ ìë¥¼ ê²ƒ
    audio = AudioSegment.from_file(input_path)
    chunks = []
    total_chunks = len(audio) // chunk_length_ms + int(len(audio) % chunk_length_ms != 0) # ë¶„í• ëœ ì²­í¬ = ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ / 10ë¶„ + (10ë¶„ ë¯¸ë§Œì˜ ë‚˜ë¨¸ì§€)
    
    base = os.path.splitext(os.path.basename(input_path))[0]
    output_dir = os.path.join("tmp_audio", base)
    os.makedirs(output_dir, exist_ok=True)

    # ì˜¤ë””ì˜¤ë¥¼ ì´ ëª‡ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆŒì§€ ê³„ì‚° í›„ì— ê° ì¡°ê° ìë¥´ê³  ì €ì¥
    for i in range(total_chunks):
        start = i * chunk_length_ms
        end = start + chunk_length_ms
        chunk = audio[start:end]
        chunk_path = os.path.join(output_dir, f"{base}_part{i+1}.wav")
        chunk.export(chunk_path, format="wav")
        chunks.append(chunk_path)

    return chunks


# 2. Whisper
def transcribe_audio(file_path):
    file_path = os.path.normpath(file_path) # ê²½ë¡œ ì•ˆì •í™”
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"íŒŒì¼ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
    
    wav_path = convert_to_wav(file_path)
    
    with open(wav_path, "rb") as f:
        response = client.audio.transcriptions.create(
            model="whisper-1",
            file=f,
            language="ko"
        )
        
    # os.remove(wav_path)
    return response.text, wav_path

# 3. í…ìŠ¤íŠ¸ chunk ë‚˜ëˆ„ê¸°
def chunk_text(text, max_chars=3000):
    return textwrap.wrap(text, max_chars)

# 4. 1ì°¨ ìš”ì•½
def summarize_chunk(chunk, index):
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ ìš”ì•½ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": f"[{index+1}] ì•„ë˜ ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n\n{chunk}"}
        ]
    )
    return response.choices[0].message.content.strip()

# 5. 2ì°¨ ìš”ì•½
def summarize_full_text(full_text):
    chunks = chunk_text(full_text)
    print(f"[ğŸ“š] í…ìŠ¤íŠ¸ {len(chunks)} ì¡°ê°ìœ¼ë¡œ ë¶„í• ë¨")

    summaries = []
    for i, chunk in enumerate(chunks):
        print(f"[ğŸ“] 1ì°¨ ìš”ì•½ ì¤‘: {i+1}/{len(chunks)}")
        summaries.append(summarize_chunk(chunk, i))

    combined = "\n".join(summaries)

    print("[ğŸ§ ] 2ì°¨ ìš”ì•½ ì¤‘...")
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ ìš”ì•½ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": f"ë‹¤ìŒ ë‚´ìš©ì„ 3ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n\n{combined}"}
        ]
    )
    return response.choices[0].message.content.strip()

# 6. í•  ì¼ ì¶”ì¶œ
def extract_tasks(summary):
    prompt = f"""
ë‹¤ìŒ ìš”ì•½ëœ íšŒì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ì•¼ í•  ì‘ì—… ëª©ë¡ì„ í•­ëª©ë³„ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.
- ê° í•­ëª©ì€ í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
- ì¶œë ¥ì€ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ í•´ì£¼ì„¸ìš”

íšŒì˜ 3ë¬¸ë‹¨ ìš”ì•½:
{summary}
"""
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ í•  ì¼ ì¶”ì¶œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content.strip()

# í† í° ìˆ˜ ê³„ì‚° í•¨ìˆ˜
def get_token_count(text, model="gpt-4"):
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

# ğŸ¯ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜
def run_pipeline(audio_path, token_limit=120000):
    print("[ğŸ”§] ì˜¤ë””ì˜¤ ë¶„í•  ì¤‘...")
    chunk_paths = split_audio(audio_path)

    wav_files_to_delete = []

    full_text = ""
    for idx, path in enumerate(chunk_paths):
        print(f"[ğŸ™ï¸] Whisper ë³€í™˜ ì¤‘... ({idx+1}/{len(chunk_paths)})")
        transcript, wav_path = transcribe_audio(path)
        full_text += transcript + "\n"
        wav_files_to_delete.append(wav_path)

    
    total_tokens = get_token_count(full_text, model="gpt-4")
    print(f"[ğŸ“] ì „ì²´ í…ìŠ¤íŠ¸ í† í° ìˆ˜: {total_tokens} tokens")

    if total_tokens > token_limit:
        raise ValueError(
            f"ìŒì„± íŒŒì¼ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. í˜„ì¬ {total_tokens} tokensë¡œ ì œí•œ {token_limit} tokensì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.\n"
            f"ì˜¤ë””ì˜¤ë¥¼ ë” ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ê±°ë‚˜ ì¼ë¶€ë§Œ ì²˜ë¦¬í•´ ì£¼ì„¸ìš”."
        )

    print("[ğŸ§ ] 3ë¬¸ë‹¨ ìš”ì•½ ì¶”ì¶œ ì¤‘...")
    summary = summarize_full_text(full_text)

    print("[ğŸ“] í•  ì¼ ëª©ë¡ ì¶”ì¶œ ì¤‘...")
    tasks = extract_tasks(summary)

    # íŒŒì¼ ì‚­ì œ 
    try:
        os.remove(audio_path)  # ì—…ë¡œë“œëœ ì›ë³¸ íŒŒì¼
        for path in wav_files_to_delete:
            os.remove(path)
        shutil.rmtree(os.path.join("tmp_audio", os.path.splitext(os.path.basename(audio_path))[0]))
        print("ğŸ§¹ ëª¨ë“  ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        print(f"[âš ï¸] ì„ì‹œ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")

    return {
        "transcript": full_text,
        "summary": summary,
        "tasks": tasks
    }
