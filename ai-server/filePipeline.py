import os
import shutil
import ffmpeg
from openai import OpenAI
from dotenv import load_dotenv 

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# 1. ìŒì„± íŒŒì¼ ì¸ì½”ë”© ë³€í™˜
def convert_to_wav(input_path: str) -> str:
    import uuid
    
    print("[íŒŒì¼ íŒŒì´í”„ë¼ì¸ ì‹œì‘]", input_path)
   

    tmp_dir = os.path.join("tmp_audio")
    os.makedirs(tmp_dir, exist_ok=True)

    unique_name = str(uuid.uuid4())[:8] + ".wav"
    output_path = os.path.join(tmp_dir, unique_name)

    print("out:: ", output_path)
    print("[convert_to_wav] input exists:", os.path.exists(input_path))

    try:
        (
            ffmpeg
            .input(input_path)
            .output(output_path, format='wav', acodec='pcm_s16le', ac=1, ar='16000')
            .overwrite_output()
            .run(quiet=True)
        )
        print("ffmeg ì‹¤í–‰ ì™„ë£Œ")
        return output_path
    except ffmpeg.Error as e:
        print("[FFmpeg ë³€í™˜ ì˜¤ë¥˜]", e.stderr.decode())
        raise RuntimeError("ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨")


# 2. Whisper ì „ì‚¬
def transcribe_audio(file_path: str):
    file_path = os.path.normpath(file_path) # ê²½ë¡œ ì•ˆì •í™”
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"íŒŒì¼ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
    
    wav_path = convert_to_wav(file_path)
    
    with open(wav_path, "rb") as f:
        response = client.audio.transcriptions.create(
            model="whisper-1",
            file=f,
            language="ko"
        )
        
    os.remove(wav_path)
    return response.text

# 2. í…ìŠ¤íŠ¸ chunk ë‚˜ëˆ„ê¸°
def chunk_text(text: str, max_chars: int = 3000) -> list:
    import textwrap
    return textwrap.wrap(text, max_chars)

# 3. 1ì°¨ ìš”ì•½
def summarize_chunk(chunk: str, index: int) -> str:
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ ìš”ì•½ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": f"[ë¶€ë¶„ {index+1}] ì•„ë˜ ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n\n{chunk}"}
        ]
    )
    return response.choices[0].message.content.strip()

# 4. 2ì°¨ ìš”ì•½
def summarize_full_text(full_text: str) -> str:
    chunks = chunk_text(full_text)
    print(f"[ğŸ“š] í…ìŠ¤íŠ¸ {len(chunks)} ì¡°ê°ìœ¼ë¡œ ë¶„í• ë¨")

    summaries = []
    for i, chunk in enumerate(chunks):
        print(f"[ğŸ“] 1ì°¨ ìš”ì•½ ì¤‘: {i+1}/{len(chunks)}")
        summaries.append(summarize_chunk(chunk, i))

    combined = "\n".join(summaries)

    print("[ğŸ§ ] 2ì°¨ ìš”ì•½ ì¤‘...")
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ ìš”ì•½ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": f"ë‹¤ìŒ ë‚´ìš©ì„ 3ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n\n{combined}"}
        ]
    )
    return response.choices[0].message.content.strip()

# 5. í•  ì¼ ì¶”ì¶œ
def extract_tasks(summary: str) -> str:
    prompt = f"""
ë‹¤ìŒ ìš”ì•½ëœ íšŒì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ì•¼ í•  ì‘ì—… ëª©ë¡ì„ í•­ëª©ë³„ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.
- ê° í•­ëª©ì€ í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
- ì¶œë ¥ì€ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ í•´ì£¼ì„¸ìš”

íšŒì˜ 3ë¬¸ë‹¨ ìš”ì•½:
{summary}
"""
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ í•  ì¼ ì¶”ì¶œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content.strip()

# ğŸ¯ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜
def run_pipeline(audio_path: str):
    
    
    full_text = ""
    
    transcript = transcribe_audio(audio_path)
    full_text += transcript + "\n"

    print("[ğŸ§ ] 3ë¬¸ë‹¨ ìš”ì•½ ì¶”ì¶œ ì¤‘...")
    summary = summarize_full_text(full_text)

    print("[ğŸ“] í•  ì¼ ëª©ë¡ ì¶”ì¶œ ì¤‘...")
    tasks = extract_tasks(summary)

    os.remove(audio_path)

    return {
        "transcript": full_text,
        "summary": summary,
        "tasks": tasks
    }
